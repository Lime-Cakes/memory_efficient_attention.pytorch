[metadata]
name = memory-efficient-attention
version = 0.0.2
author = Ryuichiro Hataya
description = A human-readable PyTorch implementation of "Self-attention Does Not Need O(n^2) Memory" (Rabe&Staats'21).
long_description = file: README.md
long_description_content_type = text/markdown
url = https://github.com/moskomule/memory_efficient_attention.pytorch
project_urls =
    Bug Tracker = https://github.com/moskomule/memory_efficient_attention.pytorch
classifiers =
    Programming Language :: Python :: 3
    License :: OSI Approved :: Apache License
    Operating System :: OS Independent

[options]
py_modules = memory_efficient_attention
python_requires = >=3.8
install_requires =
    torch
